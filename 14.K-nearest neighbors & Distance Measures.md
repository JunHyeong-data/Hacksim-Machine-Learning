# K-최근접 이웃 (K-Nearest Neighbors, KNN)

## 1. 머신러닝 방법론의 분류
분류 및 예측을 위한 머신러닝 방법은 크게 두 가지로 나뉜다.
<img width="1506" height="904" alt="image" src="https://github.com/user-attachments/assets/3816c686-2475-4359-a024-213d2348d634" />

### 1.1 모델 기반 학습 (Model-Based Learning)
학습 데이터를 이용해 모델을 먼저 생성하고, 이후 새로운 데이터가 들어오면 모델을 통해 예측한다.
- **예시:** 선형 회귀, 로지스틱 회귀, 신경망, 의사결정나무, 서포트 벡터 머신
- **과정:** 학습 데이터로 모델 학습 $\rightarrow$ 모델 완성 $\rightarrow$ 새로운 데이터 예측

### 1.2 인스턴스 기반 학습 (Instance-Based Learning)
모델을 미리 만들지 않고, 새로운 데이터가 들어올 때마다 학습 데이터를 직접 활용해 예측한다.
👉 **KNN은 인스턴스 기반 학습에 해당한다.**

---

## 2. KNN의 기본 개념
<img width="1442" height="882" alt="image" src="https://github.com/user-attachments/assets/933b3e17-0409-4ebf-b230-dc33e7bbcb05" />

KNN은 새로운 데이터가 주어졌을 때 **가장 가까운 K개의 학습 데이터(이웃)**를 기준으로 분류 또는 예측을 수행하는 알고리즘이다.
- $K$: 고려할 이웃의 개수
- “모델이 없는 모델”이라고도 불림

---

## 3. KNN의 특징 (학습 관점)
<img width="1434" height="729" alt="image" src="https://github.com/user-attachments/assets/011e3cd0-124c-4073-ab5a-995b54cd4e56" />

- **Instance-Based Learning:** 관측치 자체를 사용해 예측
- **Memory-Based Learning:** 모든 학습 데이터를 저장하고 사용
- **Lazy Learning:** 사전에 학습하지 않고, 테스트 데이터가 들어올 때 계산 수행

---

## 4. KNN 분류 (Classification)
### 4.1 기본 아이디어
1. 새로운 데이터 $x$ 선택
2. 학습 데이터와의 거리 계산
3. 가장 가까운 $K$개 이웃 선택
4. **다수결(Majority Vote)**로 클래스 결정



### 4.2 예시
<img width="1482" height="795" alt="image" src="https://github.com/user-attachments/assets/32fb3c78-8d28-4d5c-9460-762562af0bfd" />

- **$K = 1$**: 가장 가까운 1개의 데이터 클래스 사용
- **$K = 3$**: 가장 가까운 3개의 데이터 중 가장 많은 클래스 선택

---

## 5. KNN 예측 (Regression)
KNN은 연속형 $Y$값 예측에도 사용 가능하다.
<img width="1472" height="875" alt="image" src="https://github.com/user-attachments/assets/3ebd4d3f-1cf2-4cd5-ac20-704375c7d80e" />

### 5.1 기본 아이디어
- 가장 가까운 $K$개 이웃 선택 후 이웃들의 $Y$값을 이용해 예측

### 5.2 예측 방법
- **기본값: 평균(mean)**
$$\hat{y} = \frac{1}{K}\sum_{i=1}^{K} y_i$$
※ 중앙값, 최소값 등도 가능하지만 경험적으로 평균이 가장 성능이 좋음

---

## 6. 하이퍼파라미터 (Hyperparameters)
KNN에는 학습 파라미터는 없고, 하이퍼파라미터만 존재한다.
<img width="1291" height="640" alt="image" src="https://github.com/user-attachments/assets/717e238c-0902-4487-8ef5-0ec1d17a5e18" />

### 6.1 K (이웃의 개수)
<img width="1433" height="801" alt="image" src="https://github.com/user-attachments/assets/1e0a5c87-fda5-44e1-a145-84dba4fec089" />

- **$K$가 작을수록:** 로컬 패턴 반영, 과적합(overfitting) 위험
- **$K$가 클수록:** 글로벌 패턴 반영, 과소적합(underfitting) 위험

### 6.2 K 선택 방법
<img width="1441" height="1060" alt="image" src="https://github.com/user-attachments/assets/a33dbe6d-39eb-4e1e-aa64-89b591b7f98f" />

- 이론적으로 정해진 값은 없음
- Trial & Error, Grid Search
- 훈련/검증 데이터 오류를 비교하여 선택

---

## 7. 성능 평가
### 7.1 분류 문제
- **Misclassification Error**
$$\text{Error} = \frac{1}{n}\sum I(y_i \neq \hat{y}_i)$$

### 7.2 예측 문제
- **SSE (Sum of Squared Errors)**
$$\sum (y_i - \hat{y}_i)^2$$

---

## 8. K 값에 따른 성능 변화
<img width="1368" height="896" alt="image" src="https://github.com/user-attachments/assets/5802b4ad-7e93-4ba6-99fa-fefbda3cbb7c" />

- **$K \downarrow$:** Training Error $\downarrow$, Testing Error $\uparrow$ (과적합)
- **$K \uparrow$:** Training Error $\uparrow$, Testing Error $\downarrow \rightarrow$ 다시 $\uparrow$ (과소적합)
👉 **Training & Testing Error가 동시에 작은 $K$ 선택**



---

## 9. 거리 측도 (Distance Metric)
KNN의 핵심은 거리 계산이다.

### 9.1 유클리드 거리 (Euclidean Distance)
<img width="1456" height="902" alt="image" src="https://github.com/user-attachments/assets/057f69c8-c2f3-46e5-9b62-ac53b618c894" />

$$d(x,y) = \sqrt{\sum (x_i - y_i)^2}$$
- 직선 거리, 가장 많이 사용됨

### 9.2 맨하탄 거리 (Manhattan Distance)
<img width="1404" height="1016" alt="image" src="https://github.com/user-attachments/assets/9ba5cdb7-2f59-4f4b-bb65-246dcf1f4bc6" />

$$d(x,y) = \sum |x_i - y_i|$$
- 격자 이동 거리, 유클리드 거리보다 항상 크거나 같음

### 9.3 마할라노비스 거리 (Mahalanobis Distance)
<img width="1424" height="975" alt="image" src="https://github.com/user-attachments/assets/1f7ad119-672f-43be-8f14-eb21b65285c5" />
<img width="1503" height="1075" alt="image" src="https://github.com/user-attachments/assets/8fc3a570-8799-4f99-862d-1a675abca850" />

$$d(x,y) = \sqrt{(x-y)^T \Sigma^{-1}(x-y)}$$
- 분산·공분산 고려, 상관관계가 있는 데이터에 효과적

### 9.4 코릴레이션 거리 (Correlation Distance)
<img width="1485" height="988" alt="image" src="https://github.com/user-attachments/assets/eb5f803e-1e59-48c9-901b-c840676d7093" />

$$d = 1 - \text{corr}(x,y)$$
- 전체 패턴 유사도 비교, 시계열이나 신호 데이터에 적합
<img width="1466" height="1014" alt="image" src="https://github.com/user-attachments/assets/faa55127-9523-4728-b936-c5e4951c81e0" />
<img width="1514" height="1046" alt="image" src="https://github.com/user-attachments/assets/672169df-3ece-458b-b490-99d7a8b644c6" />

---

## 10. 데이터 스케일 문제
- 변수마다 스케일이 다르면 거리 왜곡이 발생하므로 **정규화 / 표준화 필수**

---

## 11. Weighted KNN
<img width="1443" height="951" alt="image" src="https://github.com/user-attachments/assets/069e289a-3563-4ed2-aa92-a03f5fbe01a5" />
<img width="1436" height="985" alt="image" src="https://github.com/user-attachments/assets/19d32f5a-78e7-408c-8428-ce4dbeecee9e" />
<img width="1460" height="1074" alt="image" src="https://github.com/user-attachments/assets/e57faf9f-c921-4d7f-863a-32c72a6b5aa3" />

- **기본 KNN:** $\hat{y} = \text{mean}(y_1, y_2, \dots, y_K)$
- **Weighted KNN:** 가까운 이웃에 더 큰 가중치 부여 (거리의 역수 등 활용)

---

## 12. KNN의 장단점
<img width="1494" height="1022" alt="image" src="https://github.com/user-attachments/assets/2a0e77cb-5e67-46db-a814-74bb3d6ba84d" />

### 장점
- 개념이 단순함
- 모델 가정이 없음 (Non-parametric)
- 비선형 패턴 처리 가능

### 단점
- 계산량이 큼 (모든 데이터 거리 계산)
- 고차원 데이터에 취약 (차원의 저주)
- $K$와 거리 선택이 어려움

---

## 13. 요약
<img width="1427" height="978" alt="image" src="https://github.com/user-attachments/assets/beee5a75-1ac5-4ab8-95a9-e2365556d900" />

- KNN은 모델을 만들지 않는 인스턴스 기반 학습
- 거리 기반 분류·예측 수행
- 핵심 결정 요소: $K$, 거리 측도
